{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8e311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from config.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import config\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aba8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to display the cells distincts and avoid cluttering of the cells we add contours to identify cell boundaries\n",
    "#this code is to identify contours in the image\n",
    "def fix_patch(patch, val):\n",
    "    patch_tmp = np.where(patch == val, patch, 0)\n",
    "    blurred_patch = gaussian_filter(patch_tmp, sigma=0.7)\n",
    "    patch_tmp = np.where((blurred_patch < int(0.9 * val)) & (blurred_patch > int(0.5 * val)), 0, 1)\n",
    "    return patch * patch_tmp\n",
    "\n",
    "\n",
    "def smart_matrix_indexing(r_min, r_max, c_min, c_max, mat):\n",
    "    row_max, col_max = np.subtract(mat.shape, (1, 1))\n",
    "    r_min = np.max([r_min - 3, 0])\n",
    "    r_max = np.min([r_max + 3, row_max])\n",
    "    c_min = np.max([c_min - 3, 0])\n",
    "    c_max = np.min([c_max + 3, col_max])\n",
    "    return r_min, r_max, c_min, c_max\n",
    "\n",
    "\n",
    "def fix_segmentation_maps(mask):\n",
    "    unique_values = np.unique(mask)\n",
    "    unique_values = unique_values[np.where(unique_values > 0)]\n",
    "    for val in unique_values:\n",
    "        r, c = np.where(mask == val)\n",
    "        r_min, r_max, c_min, c_max = smart_matrix_indexing(r.min(), r.max(), c.min(), c.max(), mask)\n",
    "        patch = mask[r_min:r_max, c_min:c_max]\n",
    "        mask[r_min:r_max, c_min:c_max] = fix_patch(patch, val)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefc1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, imagePaths, maskPaths, transforms):\n",
    "        # store the image and mask filepaths, and augmentation\n",
    "        # transforms\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of total samples contained in the dataset\n",
    "        return len(self.imagePaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab the image path from the current index\n",
    "        imagePath = self.imagePaths[idx]\n",
    "\n",
    "        # load the image from disk, swap its channels from BGR to RGB,\n",
    "        # and read the associated mask from disk in grayscale mode\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        fixed_mask = imread(self.maskPaths[idx])\n",
    "        fixed_mask = fixed_mask.astype(np.uint8)\n",
    "        #plt.imshow(fixed_mask)\n",
    "        mask_copy = cv2.imread(self.maskPaths[idx])\n",
    "        mask_copy = mask_copy.astype(np.uint8)\n",
    "        fixed_mask = fix_segmentation_maps(fixed_mask)\n",
    "        #print(fixed_mask.shape)\n",
    "        #print(np.unique(fixed_mask))\n",
    "        #plt.imshow(fixed_mask)\n",
    "\n",
    "        #bin_mask(fixed_mask,mask_copy)\n",
    "        for i in range(len(fixed_mask)):\n",
    "            for j in range(len(fixed_mask[i])):\n",
    "                if(fixed_mask[i][j] != 0):\n",
    "                    fixed_mask[i][j] = 255\n",
    "                else:\n",
    "                    fixed_mask[i][j]=0\n",
    "        #print(fixed_mask[0][0])\n",
    "\n",
    "        # check to see if we are applying any transformations\n",
    "        if self.transforms is not None:\n",
    "            # apply the transformations to both image and its mask\n",
    "            image = self.transforms(image)\n",
    "            fixed_mask = self.transforms(fixed_mask)\n",
    "        # return a tuple of the image and its mask\n",
    "        #print(np.unique(fixed_mask))\n",
    "        return (image, fixed_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8ca13",
   "metadata": {},
   "source": [
    "#img_path = config.IMAGE_DATASET_PATH\n",
    "#mask_path = config.MASK_DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154633e7",
   "metadata": {},
   "source": [
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c5f30",
   "metadata": {},
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5120e28",
   "metadata": {},
   "source": [
    "transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize((config.INPUT_IMAGE_HEIGHT, config.INPUT_IMAGE_WIDTH)),\n",
    "                                 transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db795f0",
   "metadata": {},
   "source": [
    "#transforms.ToPILImage()\n",
    "#transforms.Resize((config.INPUT_IMAGE_HEIGHT, config.INPUT_IMAGE_WIDTH))\n",
    "#transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b0748",
   "metadata": {},
   "source": [
    "imagePaths = sorted(list(paths.list_images(config.IMAGE_DATASET_PATH)))\n",
    "maskPaths = sorted(list(paths.list_images(config.MASK_DATASET_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07761134",
   "metadata": {},
   "source": [
    "len(imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed8c87",
   "metadata": {},
   "source": [
    "len(maskPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbaafd",
   "metadata": {},
   "source": [
    "trainDS = SegmentationDataset(imagePaths=imagePaths, maskPaths=maskPaths,\n",
    "    transforms=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8cab4",
   "metadata": {},
   "source": [
    "(trainDS.__getitem__(0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d4ab2",
   "metadata": {},
   "source": [
    "plt.imshow(trainDS.__getitem__(0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b2bee",
   "metadata": {},
   "source": [
    "plt.imshow(trainDS.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd477d",
   "metadata": {},
   "source": [
    "trainDS.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03eda3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
